import torch

x = torch.randn(3, requires_grad=True) 
print(x)

y = x + 2
z = y * y * 2
z = z.mean()
print(z)

z.backward() # dz/dx, z is a scalar

# if z is a vector, we need to pass a vector of ones to backward
z = y * y * 2
v = torch.ones(z.size())
z.backward(v)

print(x.grad) # dz/dx

# stop autograd from tracking history on Tensors with .requires_grad=False
x.requires_grad_(False)
# or use .detach() to get a new Tensor with the same content but that does not require gradients
y = x.detach()
# or wrap in 'with torch.no_grad():'
with torch.no_grad():
    y = x + 2
    print(y)

# A dummy example of training a model
weights = torch.ones(4, requires_grad=True)
# optimizer = torch.optim.SGD([weights], lr=0.01)
# optimizer.step()
# optimizer.zero_grad()

for epoch in range(3):
    model_output = (weights*3).sum()
    model_output.backward()
    
    print(weights.grad)
    weights.grad.zero_() # zero the gradients after updating weights
   

